digraph {
	graph [size="12,12"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2512231852336 [label="
 (16, 120, 2)" fillcolor=darkolivegreen1]
	2511812615840 [label="ViewBackward0
-------------------------
self_sym_sizes: (16, 240)"]
	2511745354336 -> 2511812615840
	2511745354336 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (16, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (128, 240)
mat2_sym_strides:       (1, 128)"]
	2511745358608 -> 2511745354336
	2511768172112 [label="fc2.bias
 (240)" fillcolor=lightblue]
	2511768172112 -> 2511745358608
	2511745358608 [label=AccumulateGrad]
	2511745354912 -> 2511745354336
	2511745354912 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2511745365520 -> 2511745354912
	2511745365520 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (16, 7680)
mat1_sym_strides:      (7680, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (7680, 128)
mat2_sym_strides:      (1, 7680)"]
	2511745353424 -> 2511745365520
	2511768161360 [label="fc1.bias
 (128)" fillcolor=lightblue]
	2511768161360 -> 2511745353424
	2511745353424 [label=AccumulateGrad]
	2511745358224 -> 2511745365520
	2511745358224 [label="ViewBackward0
-----------------------------
self_sym_sizes: (16, 64, 120)"]
	2511745354720 -> 2511745358224
	2511745354720 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2511745351936 -> 2511745354720
	2511745351936 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	2511745356976 -> 2511745351936
	2511745356976 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2511745360240 -> 2511745356976
	2511745360240 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	2511745363216 -> 2511745360240
	2511805862096 [label="conv1.weight
 (32, 1, 5)" fillcolor=lightblue]
	2511805862096 -> 2511745363216
	2511745363216 [label=AccumulateGrad]
	2511745352080 -> 2511745360240
	2511805873520 [label="conv1.bias
 (32)" fillcolor=lightblue]
	2511805873520 -> 2511745352080
	2511745352080 [label=AccumulateGrad]
	2511745364464 -> 2511745351936
	2512208774064 [label="conv2.weight
 (64, 32, 5)" fillcolor=lightblue]
	2512208774064 -> 2511745364464
	2511745364464 [label=AccumulateGrad]
	2511745353520 -> 2511745351936
	2512208783088 [label="conv2.bias
 (64)" fillcolor=lightblue]
	2512208783088 -> 2511745353520
	2511745353520 [label=AccumulateGrad]
	2511745356496 -> 2511745365520
	2511745356496 [label=TBackward0]
	2511745356784 -> 2511745356496
	2511761369872 [label="fc1.weight
 (128, 7680)" fillcolor=lightblue]
	2511761369872 -> 2511745356784
	2511745356784 [label=AccumulateGrad]
	2511745363744 -> 2511745354336
	2511745363744 [label=TBackward0]
	2511745355344 -> 2511745363744
	2511768160592 [label="fc2.weight
 (240, 128)" fillcolor=lightblue]
	2511768160592 -> 2511745355344
	2511745355344 [label=AccumulateGrad]
	2511812615840 -> 2512231852336
	2512231851760 [label="
 (16, 240)" fillcolor=darkolivegreen3]
	2511745354336 -> 2512231851760
	2512231851760 -> 2512231852336 [style=dotted]
}
