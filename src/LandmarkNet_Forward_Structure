digraph {
	graph [size="12,12"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1747388061968 [label="
 (16, 120, 2)" fillcolor=darkolivegreen1]
	1747387917376 [label="ViewBackward0
-------------------------
self_sym_sizes: (16, 240)"]
	1747387919920 -> 1747387917376
	1747387919920 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (16, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (128, 240)
mat2_sym_strides:       (1, 128)"]
	1747387917856 -> 1747387919920
	1747387210672 [label="fc2.bias
 (240)" fillcolor=lightblue]
	1747387210672 -> 1747387917856
	1747387917856 [label=AccumulateGrad]
	1747387919872 -> 1747387919920
	1747387919872 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1747387920640 -> 1747387919872
	1747387920640 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (16, 7680)
mat1_sym_strides:      (7680, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (7680, 128)
mat2_sym_strides:      (1, 7680)"]
	1747387920448 -> 1747387920640
	1747387212976 [label="fc1.bias
 (128)" fillcolor=lightblue]
	1747387212976 -> 1747387920448
	1747387920448 [label=AccumulateGrad]
	1747387920064 -> 1747387920640
	1747387920064 [label="ViewBackward0
-----------------------------
self_sym_sizes: (16, 64, 120)"]
	1747387920256 -> 1747387920064
	1747387920256 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1747387920496 -> 1747387920256
	1747387920496 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	1747387921072 -> 1747387920496
	1747387921072 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1747387919248 -> 1747387921072
	1747387919248 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	1747387919200 -> 1747387919248
	1746992410320 [label="conv1.weight
 (32, 1, 5)" fillcolor=lightblue]
	1746992410320 -> 1747387919200
	1747387919200 [label=AccumulateGrad]
	1747387919152 -> 1747387919248
	1746991847792 [label="conv1.bias
 (32)" fillcolor=lightblue]
	1746991847792 -> 1747387919152
	1747387919152 [label=AccumulateGrad]
	1747387920736 -> 1747387920496
	1747387206928 [label="conv2.weight
 (64, 32, 5)" fillcolor=lightblue]
	1747387206928 -> 1747387920736
	1747387920736 [label=AccumulateGrad]
	1747387920352 -> 1747387920496
	1747387211536 [label="conv2.bias
 (64)" fillcolor=lightblue]
	1747387211536 -> 1747387920352
	1747387920352 [label=AccumulateGrad]
	1747387920688 -> 1747387920640
	1747387920688 [label=TBackward0]
	1747387920784 -> 1747387920688
	1747387209904 [label="fc1.weight
 (128, 7680)" fillcolor=lightblue]
	1747387209904 -> 1747387920784
	1747387920784 [label=AccumulateGrad]
	1747387920016 -> 1747387919920
	1747387920016 [label=TBackward0]
	1747387920400 -> 1747387920016
	1747387218064 [label="fc2.weight
 (240, 128)" fillcolor=lightblue]
	1747387218064 -> 1747387920400
	1747387920400 [label=AccumulateGrad]
	1747387917376 -> 1747388061968
	1747388061776 [label="
 (16, 240)" fillcolor=darkolivegreen3]
	1747387919920 -> 1747388061776
	1747388061776 -> 1747388061968 [style=dotted]
}
